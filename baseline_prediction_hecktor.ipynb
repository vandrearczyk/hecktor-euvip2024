{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3PW3zfsAJthCT89ArsiRh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vandrearczyk/hecktor-euvip2024/blob/main/baseline_prediction_hecktor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNtXSLS6meoA",
        "outputId": "89db52b9-e6f4-4e61-9e19-2bb8935ead51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-survival\n",
            "  Downloading scikit_survival-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m993.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ecos in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.0.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.4.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.26.4)\n",
            "Requirement already satisfied: osqp!=0.6.0,!=0.6.1 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (0.6.7.post0)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.1.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.13.1)\n",
            "Collecting scikit-learn<1.6,>=1.4.0 (from scikit-survival)\n",
            "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival) (0.1.7.post4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.6,>=1.4.0->scikit-survival) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->scikit-survival) (1.16.0)\n",
            "Downloading scikit_survival-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn, scikit-survival\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.3.2\n",
            "    Uninstalling scikit-learn-1.3.2:\n",
            "      Successfully uninstalled scikit-learn-1.3.2\n",
            "Successfully installed scikit-learn-1.5.1 scikit-survival-0.23.0\n"
          ]
        }
      ],
      "source": [
        "! pip install scikit-survival\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sksurv.datasets import get_x_y\n",
        "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
        "from sksurv.ensemble import RandomSurvivalForest\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.utils import resample\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_features(folder_path, file_start=\"\"):\n",
        "    \"\"\"\n",
        "    Load all CSV files from a specified folder and concatenate them into a single DataFrame.\n",
        "\n",
        "    Args:\n",
        "    folder_path (str): Path to the folder containing CSV files.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Combined DataFrame from all CSV files.\n",
        "    \"\"\"\n",
        "    dfs = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.startswith(file_start) and filename.endswith(\".csv\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(file_path)\n",
        "            dfs.append(df)\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "def preprocess_data(combined_df, prefixes=None):\n",
        "    \"\"\"\n",
        "    Preprocess the combined DataFrame by keeping the first three columns and those starting with specified prefixes.\n",
        "    Then pivot the table to combine 'Modality', 'ROI', and each feature.\n",
        "\n",
        "    Args:\n",
        "    combined_df (pd.DataFrame): Combined DataFrame from multiple CSV files.\n",
        "    prefixes (list of str or None): List of prefixes to keep in the DataFrame columns.\n",
        "                                    If None, all columns are retained.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Pivoted DataFrame ready for model training.\n",
        "    \"\"\"\n",
        "    # Keep the first three columns\n",
        "    first_three_columns = combined_df.iloc[:, :3]\n",
        "\n",
        "    # If prefixes is None, keep all columns, otherwise filter columns by the specified prefixes\n",
        "    if prefixes is None:\n",
        "        filtered_df = combined_df\n",
        "    else:\n",
        "        filtered_columns = [col for col in combined_df.columns if any(col.startswith(prefix) for prefix in prefixes)]\n",
        "        filtered_df = pd.concat([first_three_columns, combined_df[filtered_columns]], axis=1)\n",
        "\n",
        "    # Melt the filtered DataFrame\n",
        "    feature_columns = [col for col in filtered_df.columns if col not in first_three_columns.columns]\n",
        "    melted_df = filtered_df.melt(id_vars=['PatientID', 'Modality', 'ROI'], value_vars=feature_columns, var_name='Feature')\n",
        "\n",
        "    # Create combined feature names\n",
        "    melted_df['Combined'] = melted_df['ROI'] + '_' + melted_df['Modality'] + '_' + melted_df['Feature']\n",
        "\n",
        "    # Pivot the DataFrame\n",
        "    pivoted_df = melted_df.pivot_table(index='PatientID', columns='Combined', values='value')\n",
        "    pivoted_df.reset_index(inplace=True)\n",
        "\n",
        "    print(\"Number of features: \", pivoted_df.shape[1])\n",
        "\n",
        "    return pivoted_df\n",
        "\n",
        "def filter_patients(pivoted_df, survival_df):\n",
        "    \"\"\"\n",
        "    Filter out patients not present in both the pivoted and survival DataFrames\n",
        "    based on the 'PatientID' column.\n",
        "\n",
        "    Args:\n",
        "    pivoted_df (pd.DataFrame): DataFrame containing patient features.\n",
        "    survival_df (pd.DataFrame): DataFrame containing patient survival data.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the filtered pivoted_df and survival_df DataFrames.\n",
        "    \"\"\"\n",
        "\n",
        "    # Identify patients to be deleted from each DataFrame\n",
        "    deleted_from_survival = set(survival_df['PatientID']) - set(pivoted_df['PatientID'])\n",
        "    deleted_from_pivoted = set(pivoted_df['PatientID']) - set(survival_df['PatientID'])\n",
        "\n",
        "    # Print details of deleted patients\n",
        "    print(\"Deleted patients from survival_df:\", len(deleted_from_survival), deleted_from_survival)\n",
        "    print(\"Deleted patients from pivoted_df:\", len(deleted_from_pivoted), deleted_from_pivoted)\n",
        "\n",
        "    # Filter DataFrames to only include matching patients\n",
        "    pivoted_df = pivoted_df[pivoted_df['PatientID'].isin(survival_df['PatientID'])]\n",
        "    survival_df = survival_df[survival_df['PatientID'].isin(pivoted_df['PatientID'])]\n",
        "\n",
        "    # Print the number of remaining patients\n",
        "    print(\"Remaining patients:\", pivoted_df.shape[0])\n",
        "\n",
        "    return pivoted_df, survival_df"
      ],
      "metadata": {
        "id": "oUJ8lUWbmhh4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload features\n",
        "if any(fn.startswith('features_album') for fn in os.listdir('.')):\n",
        "  print('Features already uploaded')\n",
        "else:\n",
        "  uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr1frdeYmia5",
        "outputId": "2f889cef-5d8b-4da2-dd85-a1d461164f9f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features already uploaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload survival_data\n",
        "if os.path.exists('hecktor2022_endpoint_training.csv'):\n",
        "  print('Survival data already uploaded')\n",
        "else:\n",
        "  uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sbz8K7rmk27",
        "outputId": "487a9f49-cbf4-42c0-bc49-1e01d3a25cd0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Survival data already uploaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "features_df = load_features(folder_path='./', file_start=\"features_album\")\n",
        "pivoted_df = preprocess_data(features_df, prefixes=['original_intensity', 'original_SUV'])\n",
        "survival_df = pd.read_csv('hecktor2022_endpoint_training.csv')"
      ],
      "metadata": {
        "id": "Lgig9UuXmmaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7461e3f5-6a77-44e1-e904-f216c3a842fa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features:  79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out patients if not present in features or survival data\n",
        "pivoted_df, survival_df = filter_patients(pivoted_df, survival_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy3GTzknmoDo",
        "outputId": "6e360f89-75ef-4f5b-b245-0314999dad38"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted patients from survival_df: 16 {'HMR-024', 'HMR-016', 'HGJ-074', 'MDA-001', 'CHUV-033', 'MDA-006', 'MDA-003', 'MDA-146', 'HMR-005', 'HMR-034', 'HMR-029', 'MDA-007', 'MDA-005', 'HGJ-073', 'CHUV-035', 'HMR-030'}\n",
            "Deleted patients from pivoted_df: 34 {'CHUP-025', 'CHUP-048', 'CHUP-041', 'CHUP-043', 'CHUP-027', 'CHUP-018', 'CHUP-062', 'CHUP-007', 'CHUP-069', 'CHUP-016', 'CHUP-055', 'CHUV-010', 'CHUV-007', 'CHUP-024', 'CHUP-012', 'CHUV-005', 'CHUP-049', 'CHUP-015', 'CHUV-003', 'CHUP-070', 'CHUP-033', 'CHUP-040', 'CHUP-039', 'CHUV-002', 'CHUP-050', 'CHUP-071', 'CHUV-011', 'CHUP-023', 'CHUP-005', 'CHUP-019', 'CHUV-008', 'CHUP-060', 'CHUP-004', 'CHUP-003'}\n",
            "Remaining patients: 472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training\n",
        "X = pivoted_df.drop(columns=['PatientID'])\n",
        "X = X.fillna(X.mean())\n",
        "y = np.array([(status, time) for status, time in zip(survival_df['Relapse'], survival_df['RFS'])],\n",
        "                dtype=[('event', 'bool'), ('time', 'float')])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(mutual_info_classif, k=10)  # Keep top 10 features\n",
        "X_train = selector.fit_transform(X_train, y_train['event'])\n",
        "X_test = selector.transform(X_test)\n",
        "print(\"Number of features after selection:\", X_test.shape[1])\n",
        "\n",
        "# Train the model\n",
        "model = RandomSurvivalForest(n_estimators=50, min_samples_split=20, min_samples_leaf=30, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "cindex_train = concordance_index_censored(y_train['event'], y_train['time'], model.predict(X_train))[0]\n",
        "cindex_test = concordance_index_censored(y_test['event'], y_test['time'], model.predict(X_test))[0]\n",
        "\n",
        "print(f'Concordance Index (Train): {cindex_train:.2f}')\n",
        "print(f'Concordance Index (Test): {cindex_test:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzOXlV2DmrzL",
        "outputId": "6da883c1-3c5b-41d5-93ae-74614a894531"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features after selection: 10\n",
            "Concordance Index (Train): 0.76\n",
            "Concordance Index (Test): 0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train another model e.g. only with clinical info and compare them statistically\n",
        "pivoted_df = preprocess_data(features_df, prefixes=['original_glcm'])\n",
        "\n",
        "# Prepare data for training\n",
        "X2 = pivoted_df.drop(columns=['PatientID'])\n",
        "X2 = X.fillna(X2.mean())\n",
        "y = np.array([(status, time) for status, time in zip(survival_df['Relapse'], survival_df['RFS'])],\n",
        "                dtype=[('event', 'bool'), ('time', 'float')])\n",
        "X_train2, X_test2, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(mutual_info_classif, k=10)  # Keep top 10 features\n",
        "X_train2 = selector.fit_transform(X_train2, y_train['event'])\n",
        "X_test2 = selector.transform(X_test2)\n",
        "print(\"Number of features after selection:\", X_test2.shape[1])\n",
        "\n",
        "# Train the model\n",
        "# model2 = RandomSurvivalForest(n_estimators=100, min_samples_split=10, min_samples_leaf=15, random_state=42)\n",
        "model2 = RandomSurvivalForest(n_estimators=50, min_samples_split=20, min_samples_leaf=30, random_state=42)\n",
        "model2.fit(X_train2, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "cindex_train2 = concordance_index_censored(y_train['event'], y_train['time'], model2.predict(X_train2))[0]\n",
        "cindex_test2 = concordance_index_censored(y_test['event'], y_test['time'], model2.predict(X_test2))[0]\n",
        "\n",
        "print(f'Concordance Index (Train): {cindex_train2:.2f}')\n",
        "print(f'Concordance Index (Test): {cindex_test2:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf9n50MHmtcK",
        "outputId": "e0370b52-1ca6-490e-b538-55f3d69130a6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features:  49\n",
            "Number of features after selection: 10\n",
            "Concordance Index (Train): 0.78\n",
            "Concordance Index (Test): 0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate C-index\n",
        "def calculate_cindex(y_true, y_pred):\n",
        "    return concordance_index_censored(y_true['event'], y_true['time'], y_pred)[0]\n",
        "\n",
        "# Resample and compute differences\n",
        "def bootstrap_cindex_difference(X_test1, X_test2, y_test, model1, model2, n_iterations=1000):\n",
        "    differences = []\n",
        "    for _ in range(n_iterations):\n",
        "        X_resampled, y_resampled = resample(X_test1, y_test, random_state=_)\n",
        "        X_resampled2, y_resampled = resample(X_test2, y_test, random_state=_)\n",
        "        cindex1 = calculate_cindex(y_resampled, model1.predict(X_resampled))\n",
        "        cindex2 = calculate_cindex(y_resampled, model2.predict(X_resampled))\n",
        "        differences.append(cindex1 - cindex2)\n",
        "    return np.array(differences)\n",
        "\n",
        "# Compute the differences\n",
        "differences = bootstrap_cindex_difference(X_test, X_test2, y_test, model, model2)\n",
        "observed_diff = cindex_test - cindex_test2\n",
        "\n",
        "# Compute p-value\n",
        "p_value = np.mean(differences >= observed_diff)\n",
        "print(f'p-value: {p_value:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "iKGV0uYQmvIm",
        "outputId": "defcf595-bae0-4129-e6c6-08a7ab9437d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RandomSurvivalForest was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 10 features, but RandomSurvivalForest is expecting 36 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-8f765cab6ea7>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Compute the differences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdifferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbootstrap_cindex_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mobserved_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcindex_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcindex_test2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-8f765cab6ea7>\u001b[0m in \u001b[0;36mbootstrap_cindex_difference\u001b[0;34m(X_test1, X_test2, y_test, model1, model2, n_iterations)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mX_resampled2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcindex1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_cindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mcindex2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_cindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdifferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcindex1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcindex2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sksurv/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mrisk\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_cumulative_hazard_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sksurv/ensemble/forest.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, predict_fn, X)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"estimators_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mforce_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    642\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    444\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 10 features, but RandomSurvivalForest is expecting 36 features as input."
          ]
        }
      ]
    }
  ]
}