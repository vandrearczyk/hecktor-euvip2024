{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8Ka2xUVqBbGPlcKpg3LYz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vandrearczyk/hecktor-euvip2024/blob/main/baseline_prediction_hecktor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfy5IDYsLAAE",
        "outputId": "44272b5f-2de3-48ac-b753-852778f2e567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-survival in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: ecos in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.0.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.4.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.26.4)\n",
            "Requirement already satisfied: osqp!=0.6.0,!=0.6.1 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (0.6.7.post0)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.1.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<1.6,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.5.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival) (0.1.7.post4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.6,>=1.4.0->scikit-survival) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->scikit-survival) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install scikit-survival\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sksurv.datasets import get_x_y\n",
        "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
        "from sksurv.ensemble import RandomSurvivalForest\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_features(folder_path):\n",
        "    \"\"\"\n",
        "    Load all CSV files from a specified folder and concatenate them into a single DataFrame.\n",
        "\n",
        "    Args:\n",
        "    folder_path (str): Path to the folder containing CSV files.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Combined DataFrame from all CSV files.\n",
        "    \"\"\"\n",
        "    dfs = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.startswith(\"features_album\") and filename.endswith(\".csv\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(file_path)\n",
        "            dfs.append(df)\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "def preprocess_data(combined_df, prefixes=None):\n",
        "    \"\"\"\n",
        "    Preprocess the combined DataFrame by keeping the first three columns and those starting with specified prefixes.\n",
        "    Then pivot the table to combine 'Modality', 'ROI', and each feature.\n",
        "\n",
        "    Args:\n",
        "    combined_df (pd.DataFrame): Combined DataFrame from multiple CSV files.\n",
        "    prefixes (list of str or None): List of prefixes to keep in the DataFrame columns.\n",
        "                                    If None, all columns are retained.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Pivoted DataFrame ready for model training.\n",
        "    \"\"\"\n",
        "    # Keep the first three columns\n",
        "    first_three_columns = combined_df.iloc[:, :3]\n",
        "\n",
        "    # If prefixes is None, keep all columns, otherwise filter columns by the specified prefixes\n",
        "    if prefixes is None:\n",
        "        filtered_df = combined_df\n",
        "    else:\n",
        "        filtered_columns = [col for col in combined_df.columns if any(col.startswith(prefix) for prefix in prefixes)]\n",
        "        filtered_df = pd.concat([first_three_columns, combined_df[filtered_columns]], axis=1)\n",
        "\n",
        "    # Melt the filtered DataFrame\n",
        "    feature_columns = [col for col in filtered_df.columns if col not in first_three_columns.columns]\n",
        "    melted_df = filtered_df.melt(id_vars=['PatientID', 'Modality', 'ROI'], value_vars=feature_columns, var_name='Feature')\n",
        "\n",
        "    # Create combined feature names\n",
        "    melted_df['Combined'] = melted_df['ROI'] + '_' + melted_df['Modality'] + '_' + melted_df['Feature']\n",
        "\n",
        "    # Pivot the DataFrame\n",
        "    pivoted_df = melted_df.pivot_table(index='PatientID', columns='Combined', values='value')\n",
        "    pivoted_df.reset_index(inplace=True)\n",
        "\n",
        "    return pivoted_df"
      ],
      "metadata": {
        "id": "o6nw6bk5KT2Z"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload features\n",
        "if any(fn.startswith('features_album') for fn in os.listdir('.')):\n",
        "  print('Features already uploaded')\n",
        "else:\n",
        "  uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ygtvRljsMszA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb561a4-e794-411c-f3c5-b7f884ab5d3a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features already uploaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload survival_data\n",
        "if os.path.exists('dummy_survival_data.csv'):\n",
        "  print('Survival data already uploaded')\n",
        "else:\n",
        "  uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Skk6sbe6LXb6",
        "outputId": "b6fe0f03-0375-4ef4-8a83-266fabc9532d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Survival data already uploaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "features_df = load_features('./')\n",
        "pivoted_df = preprocess_data(features_df, prefixes=['original_intensity'])\n",
        "survival_df = pd.read_csv('dummy_survival_data.csv')\n"
      ],
      "metadata": {
        "id": "UDe8hSbaODdf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training\n",
        "X = pivoted_df.drop(columns=['PatientID'])\n",
        "X = X.fillna(X.mean())\n",
        "y = np.array([(status, time) for status, time in zip(survival_df['SurvivalStatus'], survival_df['SurvivalTime'])],\n",
        "                dtype=[('event', 'bool'), ('time', 'float')])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = RandomSurvivalForest(n_estimators=100, min_samples_split=10, min_samples_leaf=15, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "cindex_train = concordance_index_censored(y_train['event'], y_train['time'], model.predict(X_train))[0]\n",
        "cindex_test = concordance_index_censored(y_test['event'], y_test['time'], model.predict(X_test))[0]\n",
        "\n",
        "print(f'Concordance Index (Train): {cindex_train:.2f}')\n",
        "print(f'Concordance Index (Test): {cindex_test:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr0jv6yQLBuu",
        "outputId": "01202ed7-626a-4e4e-8d3a-2cdb4c0f75de"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concordance Index (Train): 0.83\n",
            "Concordance Index (Test): 0.53\n"
          ]
        }
      ]
    }
  ]
}